---
title: "Part 4: Feature Engineering - Converting Symptoms to Numbers"
date: 2025-11-11
author: Claude
part: 4 of 9
difficulty: beginner-friendly
estimated_time: 6-8 hours (learning + implementation)
prerequisite: "Part 3 - Data Collection (500+ labeled examples ready)"
tags: [machine-learning, feature-engineering, text-processing, healthcare, beginner]
---

# Part 4: Feature Engineering - Converting Symptoms to Numbers

**Welcome to the Most Important Phase of ML!** ğŸ”¢

Many beginners think the "fancy ML algorithm" is the hard part. **That's wrong!**

The hard part (and what makes or breaks your model) is **feature engineering** - converting real-world data (text, categories, etc.) into numbers that ML algorithms can understand.

---

## ğŸ¤” What Is Feature Engineering? (ELI5)

### The Simplest Possible Explanation

**Problem**: Computers can't understand words, only numbers.

```
âŒ Computer sees: "chest pain"
   Computer thinks: "What is this?? I don't understand words!"

âœ… Computer sees: [0.82, 0.45, 0.91, ...]  
   Computer thinks: "Great! Numbers! I can do math on these!"
```

**Feature Engineering** = Converting "chest pain" into `[0.82, 0.45, 0.91, ...]`

### Real-World Analogy: Cooking

Imagine you want to teach a robot to cook. The robot can only understand numbers.

**Bad approach** (giving raw recipe):
```
Robot instruction: "Add a pinch of salt"
Robot: "Error! What is 'pinch'? What is 'salt'?"
```

**Good approach** (converting to numbers):
```
Robot instruction: [
  ingredient_salt: 1,
  amount_grams: 2.5,
  temperature_celsius: 180,
  time_minutes: 25
]
Robot: "Perfect! I can execute this!"
```

**That's feature engineering!** Converting human concepts into numbers.

---

## ğŸ“Š Why Can't ML Just Use Text Directly?

### Quick Test: Can You Do Math on Words?

Try this:
```
"chest pain" + "shortness of breath" = ???
"headache" Ã— 2 = ???
"fever" - "cough" = ???
```

You can't! **Math doesn't work on words.**

But ML algorithms are just fancy math:
```
Logistic Regression: y = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b
Random Forest: Split if feature_5 > 0.7
Neural Network: output = activation(sum(weights Ã— inputs))
```

All of these need **numbers as inputs**, not words!

### What We're Converting

In your triage system, you have:

**Text data** (can't use directly):
- "chest pain radiating to left arm"
- "shortness of breath"
- "headache lasting 3 days"

**Numbers** (can use directly):
- Age: 55
- Severity score: 77
- Duration in hours: 72

**Categories** (need conversion):
- Urgency: "urgent" â†’ ??? 
- Severity level: "high" â†’ ???
- Medical history: ["diabetes", "hypertension"] â†’ ???

Our job: Convert EVERYTHING to numbers!

---

## ğŸ¯ What You'll Learn in This Part

By the end, you'll understand:

âœ… What "features" are (inputs to ML models)
âœ… How to convert text to numbers (TF-IDF, embeddings)
âœ… How to handle categories (one-hot encoding)
âœ… How to normalize numbers (scaling)
âœ… How to build a feature extraction pipeline
âœ… Common beginner mistakes and how to avoid them

**Time**: 6-8 hours total
- Reading/understanding: 2-3 hours
- Coding: 3-4 hours
- Testing/debugging: 1-2 hours

---

## ğŸ“š Table of Contents

1. [Understanding Features](#1-understanding-features)
2. [Types of Data & How to Handle Each](#2-types-of-data--how-to-handle-each)
3. [Converting Text to Numbers (TF-IDF)](#3-converting-text-to-numbers-tf-idf)
4. [Handling Categories (One-Hot Encoding)](#4-handling-categories-one-hot-encoding)
5. [Normalizing Numbers (Scaling)](#5-normalizing-numbers-scaling)
6. [Building the Feature Pipeline](#6-building-the-feature-pipeline)
7. [Testing Your Features](#7-testing-your-features)
8. [Common Mistakes](#8-common-mistakes)

---

## 1. Understanding Features

### What Is a "Feature"?

**Simple definition**: A feature is ONE piece of information the model uses to make predictions.

**Example**: Predicting if someone has a cold

```
Features (inputs):
  Feature 1: Has fever? (yes=1, no=0)
  Feature 2: Has cough? (yes=1, no=0)
  Feature 3: Has runny nose? (yes=1, no=0)
  Feature 4: Temperature (degrees): 38.5
  Feature 5: Days since onset: 3

Label (output we want to predict):
  Has cold? (yes=1, no=0)
```

The model learns: "If Feature1=1 AND Feature2=1 AND Feature4>38, then probably has cold"

### Features in Your Triage System

Let's look at one patient case:

**Raw Data** (what you have in database):
```json
{
  "symptoms_data": [
    {"symptom_name": "chest pain", "duration": "hours"},
    {"symptom_name": "shortness of breath", "duration": "hours"}
  ],
  "urgency": "urgent",
  "patient_age": 55,
  "medical_history": ["diabetes", "hypertension"]
}
```

**Features** (what ML model needs):
```python
[
  # Symptom features (text â†’ numbers)
  0.82,  # "chest pain" score
  0.91,  # "shortness of breath" score
  0.00,  # "headache" score (not present)
  0.00,  # "fever" score (not present)
  # ... 100 more symptom features ...
  
  # Urgency features (category â†’ numbers)
  0,     # is_routine (no)
  0,     # is_soon (no)
  1,     # is_urgent (yes)
  0,     # is_emergency (no)
  
  # Demographic features (already numbers!)
  55,    # age
  
  # Medical history features (list â†’ numbers)
  1,     # has_diabetes (yes)
  1,     # has_hypertension (yes)
  0,     # has_heart_disease (no)
  0,     # has_asthma (no)
  # ... more conditions ...
]
```

This **feature vector** (list of numbers) is what the ML model sees!

---

## 2. Types of Data & How to Handle Each

### The Four Types of Data

Every piece of data fits into one of these categories:

| Type | Example | How to Convert | Difficulty |
|------|---------|----------------|------------|
| **Numerical** | Age: 55 | Already numbers! | Easy â­ |
| **Categorical** | Urgency: "urgent" | One-hot encode | Medium â­â­ |
| **Text** | "chest pain" | TF-IDF / Embeddings | Hard â­â­â­ |
| **Lists** | ["diabetes", "hypertension"] | Multi-hot encode | Medium â­â­ |

Let's tackle each type!

### Type 1: Numerical Data (Easy!)

**Already numbers** - use them directly!

```python
# In your triage data
patient_age = 55           # âœ… Already a number!
severity_score = 77        # âœ… Already a number!
red_flags_count = 2        # âœ… Already a number!
```

**That's it!** No conversion needed.

**âš ï¸ But wait...** We might need to **scale** them later (Section 5). Preview:
```python
# Age ranges: 0-100
# Severity score ranges: 0-100
# Red flags count: 0-10

# All different scales! ML models work better if everything is 0-1 range
```

### Type 2: Categorical Data (One-Hot Encoding)

**Categories** = Fixed set of options

```python
# Urgency can only be one of these:
urgency_options = ["routine", "soon", "urgent", "emergency"]

# Patient's urgency: "urgent"
```

**Problem**: Can't just use 1, 2, 3, 4 because:
```
"routine" = 1
"soon" = 2
"urgent" = 3
"emergency" = 4

ML model sees: emergency(4) = 2 Ã— soon(2) ???
This makes no sense! Emergency isn't "twice as much" as soon.
```

**Solution: One-Hot Encoding**

Convert each option to its own binary feature:

```python
# Before (one column):
urgency = "urgent"

# After (four columns):
is_routine = 0
is_soon = 0
is_urgent = 1     â† Only this one is 1
is_emergency = 0

# Result: [0, 0, 1, 0]
```

**Visual Example**:

```
Patient 1: urgency = "routine"
  â†’ [1, 0, 0, 0]
  
Patient 2: urgency = "urgent"
  â†’ [0, 0, 1, 0]
  
Patient 3: urgency = "emergency"
  â†’ [0, 0, 0, 1]
```

**Code**:
```python
from sklearn.preprocessing import OneHotEncoder

urgency_values = [["routine"], ["urgent"], ["emergency"]]
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(urgency_values)

print(encoded)
# [[1. 0. 0. 0.]   â† routine
#  [0. 0. 1. 0.]   â† urgent
#  [0. 0. 0. 1.]]  â† emergency
```

### Type 3: Text Data (TF-IDF)

**Most complex!** Converting sentences/words to numbers.

**Example**: Symptoms are text strings
```python
symptom_texts = [
  "chest pain radiating to left arm",
  "mild headache lasting 2 days",
  "shortness of breath with exercise"
]
```

We'll use **TF-IDF** (Term Frequency - Inverse Document Frequency)

**Simple explanation** of TF-IDF:

Imagine you have 3 documents:
```
Doc 1: "chest pain chest pain chest"
Doc 2: "mild headache"
Doc 3: "chest pain"
```

**TF (Term Frequency)** = How often does word appear in THIS document?
```
Doc 1: "chest" appears 3 times â†’ high TF
Doc 2: "chest" appears 0 times â†’ low TF
```

**IDF (Inverse Document Frequency)** = How rare is this word across ALL documents?
```
"chest" appears in 2 out of 3 docs â†’ common â†’ low IDF
"headache" appears in 1 out of 3 docs â†’ rare â†’ high IDF
```

**TF-IDF** = TF Ã— IDF

- Common words (like "the", "and") get low scores
- Rare, meaningful words (like "chest", "headache") get high scores

**Result**: Each document becomes a vector of numbers!

```python
from sklearn.feature_extraction.text import TfidfVectorizer

symptoms = [
  "chest pain",
  "headache mild",
  "chest pain severe"
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(symptoms)

# Result (simplified):
#           chest  pain  headache  mild  severe
# Doc 0:    0.58  0.58   0.00     0.00   0.00    â† "chest pain"
# Doc 1:    0.00  0.00   0.71     0.71   0.00    â† "headache mild"
# Doc 2:    0.50  0.50   0.00     0.00   0.71    â† "chest pain severe"
```

**Now ML can do math!**
```python
similarity = tfidf_matrix[0].dot(tfidf_matrix[2].T)
# "chest pain" vs "chest pain severe" â†’ 0.67 (similar!)

similarity = tfidf_matrix[0].dot(tfidf_matrix[1].T)
# "chest pain" vs "headache mild" â†’ 0.00 (not similar!)
```

**We'll use this for symptoms!**

### Type 4: Lists (Multi-Hot Encoding)

**Lists** = Multiple categories at once

```python
# Medical history (can have multiple conditions)
patient_history = ["diabetes", "hypertension", "asthma"]
```

**Solution**: Like one-hot, but multiple can be 1

```python
# All possible conditions
conditions = ["diabetes", "hypertension", "heart_disease", "asthma", "cancer"]

# Patient's conditions: ["diabetes", "hypertension", "asthma"]
# Encoded:
[1, 1, 0, 1, 0]
 â†‘  â†‘     â†‘
 diabetes, hypertension, asthma present
```

**Code**:
```python
from sklearn.preprocessing import MultiLabelBinarizer

all_histories = [
  ["diabetes"],
  ["hypertension", "asthma"],
  ["diabetes", "hypertension", "heart_disease"]
]

mlb = MultiLabelBinarizer()
encoded = mlb.fit_transform(all_histories)

print(mlb.classes_)
# ['asthma' 'diabetes' 'heart_disease' 'hypertension']

print(encoded)
# [[0 1 0 0]                    â† only diabetes
#  [1 0 0 1]                    â† hypertension + asthma
#  [0 1 1 1]]                   â† diabetes + hypertension + heart_disease
```

---

## 3. Converting Text to Numbers (TF-IDF)

Now let's implement TF-IDF for your symptom data!

### Step 1: Extract All Symptom Text

First, we need to combine all symptoms into one text string per patient.

**Create**: `basebackend/ml/feature_extraction.py`

```python
# ml/feature_extraction.py

"""
Feature Engineering Pipeline for Medical Triage

Converts raw patient data into numerical features for ML models.
"""

import json
import numpy as np
import pandas as pd
from typing import List, Dict, Any
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, StandardScaler


def extract_symptom_text(symptoms_data: List[Dict]) -> str:
    """
    Convert symptom dictionaries to single text string.
    
    Input:
      [
        {"symptom_name": "chest pain", "description": "radiating to arm"},
        {"symptom_name": "shortness of breath"}
      ]
    
    Output:
      "chest pain radiating to arm shortness of breath"
    
    Why? TF-IDF needs one string per patient, not a list of dicts.
    """
    
    text_parts = []
    
    for symptom in symptoms_data:
        # Add symptom name
        text_parts.append(symptom.get('symptom_name', ''))
        
        # Add description if present
        if symptom.get('description'):
            text_parts.append(symptom['description'])
        
        # Add duration as text (helps with patterns)
        if symptom.get('duration'):
            text_parts.append(f"{symptom['duration']}_duration")
        
        # Add severity if present
        if symptom.get('severity'):
            text_parts.append(f"{symptom['severity']}_severity")
    
    # Join all parts with spaces
    combined_text = ' '.join(text_parts)
    
    # Clean up: lowercase, remove extra spaces
    combined_text = combined_text.lower().strip()
    
    return combined_text


# Example usage:
if __name__ == '__main__':
    test_symptoms = [
        {
            "symptom_name": "chest pain",
            "description": "radiating to left arm",
            "duration": "hours",
            "severity": "severe"
        },
        {
            "symptom_name": "shortness of breath",
            "duration": "hours"
        }
    ]
    
    text = extract_symptom_text(test_symptoms)
    print("Extracted text:")
    print(text)
    # Output: "chest pain radiating to left arm hours_duration severe_severity shortness of breath hours_duration"
```

### Step 2: Build TF-IDF Vectorizer

Now create the TF-IDF converter:

```python
# ml/feature_extraction.py (continued)

class SymptomTfidfVectorizer:
    """
    Converts symptom text to TF-IDF numerical features.
    
    Simple explanation:
      - Learns which words are important from training data
      - Converts new symptoms to numbers based on learned vocabulary
    """
    
    def __init__(self, max_features=100):
        """
        Args:
            max_features: Maximum number of symptom words to keep
                         (keeps the 100 most common/important words)
        """
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            lowercase=True,
            stop_words='english',  # Remove "the", "and", "a", etc.
            ngram_range=(1, 2)     # Use single words AND 2-word phrases
        )
        self.is_fitted = False
    
    def fit(self, symptom_texts: List[str]):
        """
        Learn vocabulary from training data.
        
        This is called ONCE on your training data to build the vocabulary.
        
        Args:
            symptom_texts: List of combined symptom strings
                          ["chest pain", "headache mild", ...]
        """
        self.vectorizer.fit(symptom_texts)
        self.is_fitted = True
        
        print(f"âœ… Learned vocabulary of {len(self.vectorizer.vocabulary_)} symptom terms")
        print(f"Top 20 terms: {list(self.vectorizer.vocabulary_.keys())[:20]}")
    
    def transform(self, symptom_texts: List[str]) -> np.ndarray:
        """
        Convert symptom texts to TF-IDF features.
        
        Args:
            symptom_texts: List of symptom strings
        
        Returns:
            numpy array of shape (n_samples, n_features)
            Each row is one patient, each column is a symptom word score
        """
        if not self.is_fitted:
            raise ValueError("Must call fit() before transform()!")
        
        tfidf_matrix = self.vectorizer.transform(symptom_texts)
        return tfidf_matrix.toarray()  # Convert sparse matrix to normal array
    
    def fit_transform(self, symptom_texts: List[str]) -> np.ndarray:
        """Convenience method: fit and transform in one step."""
        self.fit(symptom_texts)
        return self.transform(symptom_texts)


# Example usage:
if __name__ == '__main__':
    # Sample training data
    training_symptoms = [
        "chest pain radiating to arm",
        "mild headache lasting days",
        "chest pain severe",
        "shortness of breath with chest pain",
        "headache with fever",
    ]
    
    # Create and fit vectorizer
    symptom_vectorizer = SymptomTfidfVectorizer(max_features=20)
    features = symptom_vectorizer.fit_transform(training_symptoms)
    
    print("\nTF-IDF Features shape:", features.shape)
    # (5 patients, 20 features)
    
    print("\nFirst patient features:")
    print(features[0])
    # [0.52, 0.41, 0.00, 0.61, ...] â† 20 numbers representing "chest pain radiating to arm"
    
    # Transform new patient
    new_symptoms = ["chest pain"]
    new_features = symptom_vectorizer.transform(new_symptoms)
    print("\nNew patient features:")
    print(new_features[0])
```

### Understanding the Output

**What does each number mean?**

```python
Features for "chest pain radiating to arm":
[0.52, 0.41, 0.00, 0.61, 0.00, 0.00, ...]
  â†‘     â†‘     â†‘     â†‘
  |     |     |     |
  |     |     |     â””â”€ "radiating" TF-IDF score
  |     |     â””â”€ "headache" TF-IDF score (0 = not present)
  |     â””â”€ "pain" TF-IDF score
  â””â”€ "chest" TF-IDF score
```

- **High score (0.5-1.0)**: Word is present and important
- **Zero**: Word not present
- **Low score (0.1-0.3)**: Word present but common (less important)

**Why this works for ML**:
```python
# Similar symptoms get similar feature vectors
"chest pain" â†’ [0.71, 0.71, 0.00, ...]
"chest pain severe" â†’ [0.50, 0.50, 0.00, ..., 0.71]
# First two numbers are similar! ML sees they're related.

# Different symptoms get different vectors
"headache" â†’ [0.00, 0.00, 1.00, ...]
# All different numbers! ML sees they're unrelated.
```

---


## 4. Handling Categories (One-Hot Encoding)

Now let's handle categorical variables like `urgency` and `severity_level`.

### Why We Need One-Hot Encoding (Again)

**Bad approach**:
```python
# Assigning numbers directly
urgency_map = {
  "routine": 1,
  "soon": 2,
  "urgent": 3,
  "emergency": 4
}

# ML model sees: emergency(4) = 2 Ã— soon(2)
# This implies emergency is "twice as urgent" as soon
# And urgent(3) is exactly between soon(2) and emergency(4)
# These relationships are WRONG!
```

**Good approach** (one-hot):
```python
# Each category gets its own feature
"routine" â†’ [1, 0, 0, 0]
"soon" â†’ [0, 1, 0, 0]
"urgent" â†’ [0, 0, 1, 0]
"emergency" â†’ [0, 0, 0, 1]

# Now ML sees them as completely separate, unrelated categories
# Perfect!
```

### Implementation

```python
# ml/feature_extraction.py (continued)

from sklearn.preprocessing import OneHotEncoder

class CategoricalEncoder:
    """
    One-hot encode categorical variables.
    
    Simple explanation:
      Converts categories like "urgent" to binary vectors like [0, 0, 1, 0]
    """
    
    def __init__(self, categories_list: List[str]):
        """
        Args:
            categories_list: All possible values for this category
                           E.g., ['routine', 'soon', 'urgent', 'emergency']
        """
        self.encoder = OneHotEncoder(
            categories=[categories_list],
            sparse=False,  # Return normal array, not sparse matrix
            handle_unknown='ignore'  # If we see new category, use all zeros
        )
        self.is_fitted = False
        self.categories_list = categories_list
    
    def fit(self, values: List[str]):
        """
        Learn the categories (usually just validates they match categories_list).
        
        Args:
            values: List of category values from training data
        """
        # Reshape to 2D array (required by sklearn)
        values_2d = [[v] for v in values]
        self.encoder.fit(values_2d)
        self.is_fitted = True
        
        print(f"âœ… Encoded {len(self.categories_list)} categories: {self.categories_list}")
    
    def transform(self, values: List[str]) -> np.ndarray:
        """
        Convert category values to one-hot vectors.
        
        Args:
            values: List of category values
        
        Returns:
            numpy array of shape (n_samples, n_categories)
        """
        if not self.is_fitted:
            raise ValueError("Must call fit() before transform()!")
        
        values_2d = [[v] for v in values]
        return self.encoder.transform(values_2d)
    
    def fit_transform(self, values: List[str]) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(values)
        return self.transform(values)


# Example usage:
if __name__ == '__main__':
    # Define all possible urgency levels
    urgency_categories = ['routine', 'soon', 'urgent', 'emergency']
    
    # Training data
    training_urgencies = ['urgent', 'routine', 'emergency', 'soon', 'urgent']
    
    # Create encoder
    urgency_encoder = CategoricalEncoder(urgency_categories)
    encoded = urgency_encoder.fit_transform(training_urgencies)
    
    print("Urgency encoding:")
    print(encoded)
    # [[0. 0. 1. 0.]   â† urgent
    #  [1. 0. 0. 0.]   â† routine
    #  [0. 0. 0. 1.]   â† emergency
    #  [0. 1. 0. 0.]   â† soon
    #  [0. 0. 1. 0.]]  â† urgent
    
    # Transform new data
    new_urgencies = ['emergency', 'routine']
    new_encoded = urgency_encoder.transform(new_urgencies)
    print("\nNew urgencies encoded:")
    print(new_encoded)
    # [[0. 0. 0. 1.]   â† emergency
    #  [1. 0. 0. 0.]]  â† routine
```

### Handling Medical History (Multi-Hot)

Medical history is a **list** of conditions, so we use **MultiLabelBinarizer**:

```python
# ml/feature_extraction.py (continued)

from sklearn.preprocessing import MultiLabelBinarizer

class MedicalHistoryEncoder:
    """
    Encode medical history (list of conditions) to multi-hot vectors.
    
    Simple explanation:
      Patient can have multiple conditions at once:
        ["diabetes", "hypertension"] â†’ [1, 1, 0, 0, 0]
                                         â†‘  â†‘
                                    diabetes, hypertension present
    """
    
    def __init__(self, all_conditions: List[str] = None):
        """
        Args:
            all_conditions: List of all possible medical conditions
                          If None, will learn from data
        """
        self.mlb = MultiLabelBinarizer(classes=all_conditions)
        self.is_fitted = False
        self.all_conditions = all_conditions
    
    def fit(self, medical_histories: List[List[str]]):
        """
        Learn all possible conditions from training data.
        
        Args:
            medical_histories: List of condition lists
                             [["diabetes"], ["hypertension", "asthma"], ...]
        """
        self.mlb.fit(medical_histories)
        self.is_fitted = True
        self.all_conditions = list(self.mlb.classes_)
        
        print(f"âœ… Encoded {len(self.all_conditions)} conditions: {self.all_conditions}")
    
    def transform(self, medical_histories: List[List[str]]) -> np.ndarray:
        """
        Convert medical history lists to multi-hot vectors.
        
        Args:
            medical_histories: List of condition lists
        
        Returns:
            numpy array of shape (n_samples, n_conditions)
        """
        if not self.is_fitted:
            raise ValueError("Must call fit() before transform()!")
        
        return self.mlb.transform(medical_histories)
    
    def fit_transform(self, medical_histories: List[List[str]]) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(medical_histories)
        return self.transform(medical_histories)


# Example usage:
if __name__ == '__main__':
    # Training data: each patient's medical history
    training_histories = [
        ["diabetes"],
        ["hypertension", "asthma"],
        ["diabetes", "hypertension"],
        [],  # No medical history
        ["diabetes", "heart disease"]
    ]
    
    # Create encoder
    history_encoder = MedicalHistoryEncoder()
    encoded = history_encoder.fit_transform(training_histories)
    
    print("Medical history encoding:")
    print("Conditions:", history_encoder.all_conditions)
    # ['asthma', 'diabetes', 'heart disease', 'hypertension']
    
    print("\nEncoded vectors:")
    print(encoded)
    # [[0 1 0 0]   â† only diabetes
    #  [1 0 0 1]   â† asthma + hypertension
    #  [0 1 0 1]   â† diabetes + hypertension
    #  [0 0 0 0]   â† no conditions
    #  [0 1 1 0]]  â† diabetes + heart disease
```

---

## 5. Normalizing Numbers (Scaling)

### Why Scaling Matters

**Problem**: Different features have different scales

```python
Features without scaling:
  age: 55             (range: 0-100)
  severity_score: 77  (range: 0-100)
  symptom_count: 3    (range: 0-20)
  red_flags: 0        (range: 0-10)

When ML does math:
  feature1 + feature2 = 55 + 77 = 132
  
  But age(55) dominates symptom_count(3)!
  The model thinks age is "more important" just because the numbers are bigger.
```

**Solution**: Scale everything to 0-1 range (or -1 to 1)

```python
Features after scaling:
  age: 0.55           (55/100 = 0.55)
  severity_score: 0.77  (77/100 = 0.77)
  symptom_count: 0.15   (3/20 = 0.15)
  red_flags: 0.0        (0/10 = 0.0)

Now all features are on same scale!
ML can learn their TRUE importance, not based on number size.
```

### Two Types of Scaling

**1. Min-Max Scaling** (normalize to 0-1):
```python
scaled_value = (value - min) / (max - min)

Example:
  age = 55, min_age = 0, max_age = 100
  scaled_age = (55 - 0) / (100 - 0) = 0.55
```

**2. Standard Scaling** (normalize to mean=0, std=1):
```python
scaled_value = (value - mean) / std

Example:
  age = 55, mean_age = 45, std_age = 15
  scaled_age = (55 - 45) / 15 = 0.67
```

**Which to use?**
- **Min-Max**: When you know the min/max (like age: 0-120)
- **Standard**: When you don't know bounds (most cases)

We'll use **Standard Scaling** (more robust to outliers).

### Implementation

```python
# ml/feature_extraction.py (continued)

from sklearn.preprocessing import StandardScaler

class NumericalScaler:
    """
    Scale numerical features to mean=0, std=1.
    
    Simple explanation:
      Makes all numbers roughly between -3 and +3, centered at 0.
      This helps ML algorithms learn better.
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False
    
    def fit(self, numerical_values: np.ndarray):
        """
        Learn mean and standard deviation from training data.
        
        Args:
            numerical_values: 2D array of shape (n_samples, n_features)
                            Each row is one patient, each column is a number
        """
        self.scaler.fit(numerical_values)
        self.is_fitted = True
        
        print(f"âœ… Learned scaling for {numerical_values.shape[1]} numerical features")
        print(f"   Means: {self.scaler.mean_}")
        print(f"   Stds: {self.scaler.scale_}")
    
    def transform(self, numerical_values: np.ndarray) -> np.ndarray:
        """
        Scale numerical values using learned mean/std.
        
        Args:
            numerical_values: 2D array of numbers
        
        Returns:
            Scaled array (same shape)
        """
        if not self.is_fitted:
            raise ValueError("Must call fit() before transform()!")
        
        return self.scaler.transform(numerical_values)
    
    def fit_transform(self, numerical_values: np.ndarray) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(numerical_values)
        return self.transform(numerical_values)


# Example usage:
if __name__ == '__main__':
    # Training data: [age, severity_score, symptom_count]
    training_numbers = np.array([
        [25, 15, 1],   # Young patient, low severity, 1 symptom
        [55, 77, 3],   # Middle-aged, high severity, 3 symptoms
        [72, 45, 2],   # Elderly, medium severity, 2 symptoms
        [30, 20, 1],
        [60, 80, 4]
    ])
    
    # Create scaler
    scaler = NumericalScaler()
    scaled = scaler.fit_transform(training_numbers)
    
    print("Original values:")
    print(training_numbers)
    
    print("\nScaled values:")
    print(scaled)
    # Approximately:
    # [[-1.2, -1.4, -1.2]   â† age/severity/symptoms all below average
    #  [ 0.3,  0.8,  0.4]   â† middle-aged, high severity
    #  [ 1.2,  0.2,  0.0]   â† elderly
    #  ...
    ```

**Notice**: After scaling, all values are roughly -2 to +2 range, centered around 0!

---

## 6. Building the Feature Pipeline

Now we combine everything into ONE pipeline that handles all transformations!

### The Complete Feature Extractor

```python
# ml/feature_extraction.py (continued)

class TriageFeatureExtractor:
    """
    Complete feature extraction pipeline for triage ML.
    
    Takes raw triage data â†’ Outputs numerical feature vectors ready for ML.
    
    Handles:
      - Symptom text â†’ TF-IDF
      - Urgency â†’ One-hot encoding
      - Medical history â†’ Multi-hot encoding
      - Age, severity score â†’ Scaling
    """
    
    def __init__(self, max_symptom_features=100):
        # Component 1: Symptom text vectorizer
        self.symptom_vectorizer = SymptomTfidfVectorizer(max_features=max_symptom_features)
        
        # Component 2: Urgency encoder
        self.urgency_encoder = CategoricalEncoder(['routine', 'soon', 'urgent', 'emergency'])
        
        # Component 3: Medical history encoder
        self.history_encoder = MedicalHistoryEncoder()
        
        # Component 4: Numerical scaler
        self.numerical_scaler = NumericalScaler()
        
        self.is_fitted = False
    
    def fit(self, triage_records: List[Dict]):
        """
        Learn all transformations from training data.
        
        Args:
            triage_records: List of triage dictionaries from database
                          [
                            {
                              "symptoms_data": [...],
                              "urgency": "urgent",
                              "patient_age": 55,
                              "medical_history": ["diabetes"],
                              ...
                            },
                            ...
                          ]
        """
        print("="*60)
        print("FITTING FEATURE EXTRACTOR")
        print("="*60)
        
        # Extract each component's data
        symptom_texts = []
        urgencies = []
        histories = []
        numerical_values = []
        
        for record in triage_records:
            # 1. Symptom text
            symptom_text = extract_symptom_text(record['symptoms_data'])
            symptom_texts.append(symptom_text)
            
            # 2. Urgency
            urgencies.append(record['urgency'])
            
            # 3. Medical history
            histories.append(record.get('medical_history', []))
            
            # 4. Numerical features: [age, severity_score]
            numerical_values.append([
                record.get('patient_age', 40),  # Default age if missing
                record.get('severity_score', 0)
            ])
        
        # Convert to numpy array
        numerical_values = np.array(numerical_values)
        
        # Fit each component
        print("\n1. Fitting symptom vectorizer...")
        self.symptom_vectorizer.fit(symptom_texts)
        
        print("\n2. Fitting urgency encoder...")
        self.urgency_encoder.fit(urgencies)
        
        print("\n3. Fitting medical history encoder...")
        self.history_encoder.fit(histories)
        
        print("\n4. Fitting numerical scaler...")
        self.numerical_scaler.fit(numerical_values)
        
        self.is_fitted = True
        print("\n" + "="*60)
        print("âœ… FEATURE EXTRACTOR READY!")
        print("="*60)
    
    def transform(self, triage_records: List[Dict]) -> np.ndarray:
        """
        Transform triage records to feature vectors.
        
        Args:
            triage_records: List of triage dictionaries
        
        Returns:
            numpy array of shape (n_samples, n_total_features)
            Each row is one complete feature vector for a patient
        """
        if not self.is_fitted:
            raise ValueError("Must call fit() before transform()!")
        
        # Extract data
        symptom_texts = []
        urgencies = []
        histories = []
        numerical_values = []
        
        for record in triage_records:
            symptom_texts.append(extract_symptom_text(record['symptoms_data']))
            urgencies.append(record['urgency'])
            histories.append(record.get('medical_history', []))
            numerical_values.append([
                record.get('patient_age', 40),
                record.get('severity_score', 0)
            ])
        
        numerical_values = np.array(numerical_values)
        
        # Transform each component
        symptom_features = self.symptom_vectorizer.transform(symptom_texts)
        urgency_features = self.urgency_encoder.transform(urgencies)
        history_features = self.history_encoder.transform(histories)
        numerical_features = self.numerical_scaler.transform(numerical_values)
        
        # Concatenate all features horizontally
        all_features = np.hstack([
            symptom_features,   # e.g., 100 features
            urgency_features,   # 4 features
            history_features,   # e.g., 10 features
            numerical_features  # 2 features
        ])
        # Total: 116 features per patient
        
        return all_features
    
    def fit_transform(self, triage_records: List[Dict]) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(triage_records)
        return self.transform(triage_records)
    
    def get_feature_names(self) -> List[str]:
        """
        Get human-readable feature names.
        Useful for debugging and understanding which features matter.
        """
        names = []
        
        # Symptom features
        symptom_vocab = self.symptom_vectorizer.vectorizer.get_feature_names_out()
        names.extend([f"symptom_{word}" for word in symptom_vocab])
        
        # Urgency features
        names.extend(['urgency_routine', 'urgency_soon', 'urgency_urgent', 'urgency_emergency'])
        
        # Medical history features
        for condition in self.history_encoder.all_conditions:
            names.append(f"history_{condition}")
        
        # Numerical features
        names.extend(['age_scaled', 'severity_score_scaled'])
        
        return names
```

### Example: Complete Pipeline

```python
# Example usage of complete pipeline

if __name__ == '__main__':
    # Sample training data (from your TriageLog table)
    training_data = [
        {
            "symptoms_data": [
                {"symptom_name": "chest pain", "duration": "hours"},
                {"symptom_name": "shortness of breath"}
            ],
            "urgency": "urgent",
            "patient_age": 55,
            "medical_history": ["diabetes", "hypertension"],
            "severity_score": 77
        },
        {
            "symptoms_data": [
                {"symptom_name": "mild headache", "duration": "days"}
            ],
            "urgency": "routine",
            "patient_age": 28,
            "medical_history": [],
            "severity_score": 15
        },
        {
            "symptoms_data": [
                {"symptom_name": "fever", "duration": "days"},
                {"symptom_name": "cough"}
            ],
            "urgency": "soon",
            "patient_age": 45,
            "medical_history": ["asthma"],
            "severity_score": 35
        }
    ]
    
    # Create and fit extractor
    extractor = TriageFeatureExtractor(max_symptom_features=50)
    features = extractor.fit_transform(training_data)
    
    print("\n" + "="*60)
    print("FEATURE EXTRACTION RESULTS")
    print("="*60)
    
    print(f"\nFeature matrix shape: {features.shape}")
    # (3 patients, ~66 features total)
    
    print(f"\nFeature names (first 10): {extractor.get_feature_names()[:10]}")
    
    print(f"\nPatient 1 features (first 20):")
    print(features[0, :20])
    
    # Transform new patient
    new_patient = {
        "symptoms_data": [{"symptom_name": "chest pain"}],
        "urgency": "emergency",
        "patient_age": 62,
        "medical_history": ["diabetes"],
        "severity_score": 85
    }
    
    new_features = extractor.transform([new_patient])
    print(f"\nNew patient features (first 20):")
    print(new_features[0, :20])
```

**Output**:
```
==============================================================
FITTING FEATURE EXTRACTOR
==============================================================

1. Fitting symptom vectorizer...
âœ… Learned vocabulary of 12 symptom terms

2. Fitting urgency encoder...
âœ… Encoded 4 categories: ['routine', 'soon', 'urgent', 'emergency']

3. Fitting medical history encoder...
âœ… Encoded 3 conditions: ['asthma', 'diabetes', 'hypertension']

4. Fitting numerical scaler...
âœ… Learned scaling for 2 numerical features
   Means: [42.67 42.33]
   Stds: [13.5 31.01]

==============================================================
âœ… FEATURE EXTRACTOR READY!
==============================================================

Feature matrix shape: (3, 19)
# 12 symptom features + 4 urgency + 3 history + 2 numerical = 21 total

Patient 1 features (first 20):
[0.58 0.  0.58 0.47 0.  0.  ... 0.  0.  1.  0.  0.  1.  1.  0.92 1.12]
 â†‘                                   â†‘  â†‘       â†‘  â†‘  â†‘
 chest                               urgent     diabetes, hypertension
 pain                                           present    age,severity
 breath                                                    (scaled)
```

---


## 7. Testing Your Features

Before using features for ML training, we need to verify they're correct!

### Test 1: Shape Check

```python
# ml/tests/test_features.py

def test_feature_shapes():
    """Verify feature dimensions are correct."""
    
    # Create sample data
    sample_data = [
        {
            "symptoms_data": [{"symptom_name": "headache"}],
            "urgency": "routine",
            "patient_age": 30,
            "medical_history": [],
            "severity_score": 20
        },
        {
            "symptoms_data": [{"symptom_name": "chest pain"}],
            "urgency": "urgent",
            "patient_age": 60,
            "medical_history": ["diabetes"],
            "severity_score": 75
        }
    ]
    
    # Extract features
    extractor = TriageFeatureExtractor(max_symptom_features=50)
    features = extractor.fit_transform(sample_data)
    
    # Check shape
    n_patients = 2
    expected_features = (
        50 +  # Symptom TF-IDF features
        4 +   # Urgency one-hot (4 categories)
        1 +   # Medical history (diabetes appeared once)
        2     # Numerical (age, severity_score)
    )
    
    assert features.shape == (n_patients, expected_features), \
        f"Wrong shape! Got {features.shape}, expected ({n_patients}, {expected_features})"
    
    print("âœ… Test 1 PASSED: Feature shape is correct")


if __name__ == '__main__':
    test_feature_shapes()
```

### Test 2: Range Check

```python
def test_feature_ranges():
    """Verify features are in expected ranges."""
    
    sample_data = [
        {
            "symptoms_data": [{"symptom_name": "headache"}],
            "urgency": "routine",
            "patient_age": 30,
            "medical_history": [],
            "severity_score": 20
        }
    ]
    
    extractor = TriageFeatureExtractor()
    features = extractor.fit_transform(sample_data)
    
    # TF-IDF should be 0-1
    tfidf_features = features[0, :100]  # First 100 are symptoms
    assert np.all(tfidf_features >= 0) and np.all(tfidf_features <= 1), \
        "TF-IDF values should be 0-1"
    
    # One-hot should be 0 or 1
    onehot_features = features[0, 100:104]  # Next 4 are urgency
    assert np.all(np.isin(onehot_features, [0, 1])), \
        "One-hot values should be 0 or 1"
    
    # Scaled values should be roughly -3 to +3
    scaled_features = features[0, -2:]  # Last 2 are age/severity (scaled)
    assert np.all(scaled_features >= -5) and np.all(scaled_features <= 5), \
        "Scaled values should be roughly -3 to +3 (allowing -5 to +5 for outliers)"
    
    print("âœ… Test 2 PASSED: Feature ranges are correct")


if __name__ == '__main__':
    test_feature_ranges()
```

### Test 3: Consistency Check

```python
def test_feature_consistency():
    """Same input should produce same features."""
    
    sample_data = [
        {
            "symptoms_data": [{"symptom_name": "headache"}],
            "urgency": "routine",
            "patient_age": 30,
            "medical_history": [],
            "severity_score": 20
        }
    ]
    
    # Extract features twice
    extractor = TriageFeatureExtractor()
    features1 = extractor.fit_transform(sample_data)
    features2 = extractor.transform(sample_data)  # Transform again
    
    # Should be identical
    assert np.allclose(features1, features2), \
        "Same input should produce same features!"
    
    print("âœ… Test 3 PASSED: Features are consistent")


if __name__ == '__main__':
    test_feature_consistency()
```

### Test 4: Similarity Check

```python
def test_feature_similarity():
    """Similar patients should have similar features."""
    
    # Two patients with chest pain
    patient1 = {
        "symptoms_data": [{"symptom_name": "chest pain"}],
        "urgency": "urgent",
        "patient_age": 55,
        "medical_history": ["diabetes"],
        "severity_score": 77
    }
    
    patient2 = {
        "symptoms_data": [{"symptom_name": "chest pain"}],
        "urgency": "urgent",
        "patient_age": 60,
        "medical_history": ["diabetes"],
        "severity_score": 80
    }
    
    # One patient with headache
    patient3 = {
        "symptoms_data": [{"symptom_name": "headache"}],
        "urgency": "routine",
        "patient_age": 25,
        "medical_history": [],
        "severity_score": 15
    }
    
    extractor = TriageFeatureExtractor()
    features = extractor.fit_transform([patient1, patient2, patient3])
    
    # Calculate similarity using cosine similarity
    from sklearn.metrics.pairwise import cosine_similarity
    
    sim_1_2 = cosine_similarity([features[0]], [features[1]])[0][0]  # chest pain vs chest pain
    sim_1_3 = cosine_similarity([features[0]], [features[2]])[0][0]  # chest pain vs headache
    
    print(f"Similarity (chest pain vs chest pain): {sim_1_2:.3f}")
    print(f"Similarity (chest pain vs headache): {sim_1_3:.3f}")
    
    # Chest pain patients should be more similar to each other
    assert sim_1_2 > sim_1_3, \
        "Similar patients should have higher similarity score!"
    
    print("âœ… Test 4 PASSED: Similar patients have similar features")


if __name__ == '__main__':
    test_feature_similarity()
```

### Run All Tests

```python
# ml/tests/test_features.py (complete)

def run_all_tests():
    """Run all feature engineering tests."""
    
    print("\n" + "="*60)
    print("RUNNING FEATURE ENGINEERING TESTS")
    print("="*60 + "\n")
    
    try:
        test_feature_shapes()
        test_feature_ranges()
        test_feature_consistency()
        test_feature_similarity()
        
        print("\n" + "="*60)
        print("ğŸ‰ ALL TESTS PASSED!")
        print("="*60 + "\n")
        
    except AssertionError as e:
        print(f"\nâŒ TEST FAILED: {e}\n")


if __name__ == '__main__':
    run_all_tests()
```

**Run tests**:
```bash
cd /Users/new/phbfinal/basebackend
python ml/tests/test_features.py
```

---

## 8. Common Mistakes (And How to Avoid Them)

### Mistake 1: Fitting on Test Data ğŸš«

**WRONG**:
```python
# Extract features from test data
test_features = extractor.fit_transform(test_data)  # âŒ WRONG!

# This "cheats" by learning from test data vocabulary/scaling
```

**CORRECT**:
```python
# Fit on training data ONLY
extractor.fit(training_data)

# Transform both training and test
train_features = extractor.transform(training_data)
test_features = extractor.transform(test_data)  # âœ… CORRECT!
```

**Why?** The model should never see test data during training. Fitting on test data causes "data leakage" and inflates accuracy.

### Mistake 2: Forgetting to Scale ğŸš«

**WRONG**:
```python
# Using raw numbers without scaling
features = [
  age,              # 55 (range: 0-100)
  symptom_count,    # 3 (range: 0-20)
  severity_score    # 77 (range: 0-100)
]

# Model thinks age/severity are "more important" because numbers are bigger!
```

**CORRECT**:
```python
# Scale all numerical features
scaler = StandardScaler()
scaler.fit(training_numbers)
features = scaler.transform(numbers)

# Now: age=0.3, symptom_count=0.2, severity=0.7 (all same scale)
```

### Mistake 3: Using Numbers for Categories ğŸš«

**WRONG**:
```python
urgency_map = {"routine": 1, "soon": 2, "urgent": 3, "emergency": 4}
urgency_feature = urgency_map[urgency]  # âŒ Implies ordering!
```

**CORRECT**:
```python
# Use one-hot encoding
urgency_encoder = CategoricalEncoder(['routine', 'soon', 'urgent', 'emergency'])
urgency_features = urgency_encoder.transform([urgency])
# Result: [0, 0, 1, 0] for "urgent"
```

### Mistake 4: Not Handling Missing Data ğŸš«

**WRONG**:
```python
age = patient.get('patient_age')  # Could be None!
features.append(age)  # âŒ Crashes if age is None
```

**CORRECT**:
```python
# Option 1: Use default value
age = patient.get('patient_age', 40)  # Default to 40 if missing

# Option 2: Add "is_missing" feature
age = patient.get('patient_age', 0)
age_is_missing = 1 if patient.get('patient_age') is None else 0
features.extend([age, age_is_missing])
```

### Mistake 5: Too Many Features ğŸš«

**PROBLEM**:
```python
# Using every possible word as a feature
vectorizer = TfidfVectorizer(max_features=10000)  # Way too many!

# Result: 10,000 features from only 500 training examples
# Model overfits - learns noise, not patterns
```

**CORRECT**:
```python
# Limit to most important features
vectorizer = TfidfVectorizer(max_features=100)  # Reasonable!

# Rule of thumb: n_features < n_samples / 10
# 500 samples â†’ max 50 features per component
```

### Mistake 6: Forgetting to Save Fitted Extractors ğŸš«

**PROBLEM**:
```python
# Train model
extractor = TriageFeatureExtractor()
features = extractor.fit_transform(training_data)
model.fit(features, labels)

# Later, in production...
new_extractor = TriageFeatureExtractor()  # âŒ New extractor!
new_features = new_extractor.fit_transform([new_patient])  # âŒ Different vocab!
prediction = model.predict(new_features)  # âŒ Wrong! Features don't match!
```

**CORRECT**:
```python
import pickle

# Save fitted extractor
with open('feature_extractor.pkl', 'wb') as f:
    pickle.dump(extractor, f)

# Later, in production...
with open('feature_extractor.pkl', 'rb') as f:
    extractor = pickle.load(f)  # âœ… Same extractor!

new_features = extractor.transform([new_patient])  # âœ… Same transformation!
prediction = model.predict(new_features)  # âœ… Correct!
```

---

## 9. Summary & Next Steps

### What You Accomplished ğŸ‰

âœ… **Understood feature engineering**: Converting real data to numbers for ML
âœ… **Learned 4 data types**: Numerical, categorical, text, lists
âœ… **Implemented TF-IDF**: Converting symptom text to numbers
âœ… **Implemented one-hot encoding**: Converting categories to binary vectors
âœ… **Implemented scaling**: Normalizing numbers to same range
âœ… **Built complete pipeline**: End-to-end feature extraction
âœ… **Tested features**: Verified correctness with automated tests
âœ… **Avoided common mistakes**: Learned what NOT to do

### Key Takeaways for Beginners

**1. ML needs numbers, not words**
   - Can't do math on "chest pain"
   - Must convert to numbers like [0.82, 0.45, ...]

**2. Different data types need different conversions**
   - Numerical â†’ Scaling
   - Categories â†’ One-hot encoding
   - Text â†’ TF-IDF
   - Lists â†’ Multi-hot encoding

**3. Feature engineering is 70% of ML success**
   - Bad features â†’ Bad model (even with fancy algorithm)
   - Good features â†’ Good model (even with simple algorithm)

**4. Always fit on training data only!**
   - Fit extractors on training set
   - Transform both training and test sets
   - Never fit on test data (data leakage!)

**5. Test your features!**
   - Check shapes, ranges, consistency
   - Verify similar patients have similar features

### Real-World Feature Vector

Let's see what one complete feature vector looks like:

```python
Patient: 55M, chest pain + shortness of breath, urgent, diabetic

Feature Vector (length 116):
[
  # Symptom features (100 values)
  0.82,  # "chest"
  0.82,  # "pain"
  0.91,  # "shortness"
  0.91,  # "breath"
  0.00,  # "headache" (not present)
  0.00,  # "fever" (not present)
  ... 94 more symptom features ...
  
  # Urgency features (4 values)
  0,     # not routine
  0,     # not soon
  1,     # IS urgent
  0,     # not emergency
  
  # Medical history features (10 values)
  1,     # HAS diabetes
  0,     # no hypertension
  0,     # no heart disease
  ... 7 more conditions ...
  
  # Numerical features (2 values)
  0.75,  # age (scaled)
  1.13   # severity_score (scaled)
]

â†’ This entire vector gets fed to the ML model!
```

### Checklist Before Moving to Phase 4

- [ ] Understand what features are (inputs to ML models)
- [ ] Can explain TF-IDF in simple terms
- [ ] Can explain one-hot encoding
- [ ] Built and tested TriageFeatureExtractor
- [ ] All tests pass
- [ ] Understand common mistakes
- [ ] Have 500+ labeled triage records ready to extract features from

### What's Next?

**Phase 4: Model Development & Selection** ğŸ¤–
- Learn about different ML algorithms
- Understand when to use each algorithm
- Train your first ML model!
- Compare model performance
- Select the best model for production

**File**: `ml-triage-part5-phase4-model-development.md`

**Key question Phase 4 answers**: "Which ML algorithm should I use, and how do I train it?"

---

## 10. Quick Reference

### Key Functions

```python
# Extract symptom text
text = extract_symptom_text(symptoms_data)

# TF-IDF vectorization
vectorizer = SymptomTfidfVectorizer(max_features=100)
vectorizer.fit(symptom_texts)
features = vectorizer.transform(new_texts)

# One-hot encoding
encoder = CategoricalEncoder(['routine', 'soon', 'urgent', 'emergency'])
encoder.fit(urgencies)
encoded = encoder.transform(new_urgencies)

# Multi-hot encoding (medical history)
mlb = MedicalHistoryEncoder()
mlb.fit(histories)
encoded = mlb.transform(new_histories)

# Scaling
scaler = NumericalScaler()
scaler.fit(numbers)
scaled = scaler.transform(new_numbers)

# Complete pipeline
extractor = TriageFeatureExtractor()
extractor.fit(training_data)
features = extractor.transform(new_data)
```

### Feature Dimensions

For typical triage data:

| Component | # Features | Notes |
|-----------|------------|-------|
| Symptoms (TF-IDF) | 50-100 | Most important words |
| Urgency (one-hot) | 4 | routine, soon, urgent, emergency |
| Medical history (multi-hot) | 10-20 | Common conditions |
| Numerical (scaled) | 2-5 | age, severity_score, etc. |
| **TOTAL** | **66-129** | Final feature vector length |

### Saving/Loading Extractors

```python
import pickle

# Save
with open('feature_extractor.pkl', 'wb') as f:
    pickle.dump(extractor, f)

# Load
with open('feature_extractor.pkl', 'rb') as f:
    extractor = pickle.load(f)
```

---

## ğŸ“š Additional Resources

### For Learning More

**Videos**:
- "Feature Engineering for Machine Learning" by Coursera (Google)
- StatQuest: "TF-IDF Clearly Explained" (YouTube)
- StatQuest: "One-Hot Encoding" (YouTube)

**Books**:
- "Feature Engineering for Machine Learning" by Alice Zheng & Amanda Casari
- "Python Feature Engineering Cookbook" by Soledad Galli

**Practice**:
- Kaggle "Feature Engineering" course (FREE)
- Try feature engineering on other datasets (Titanic, House Prices)

---

**Author**: Claude (AI Assistant)  
**Created**: 2025-11-11  
**Part**: 4 of 9  
**Status**: Complete âœ…  
**Estimated Time**: 6-8 hours (learning + implementation)  
**Prerequisite**: Part 3 - Data Collection (500+ labeled examples)

---

**Ready for Phase 4?** Head to [Part 5: Model Development](./ml-triage-part5-phase4-model-development.md) to train your first ML model! ğŸš€

**Need to review?** Go back to the [Index](./ml-triage-system-index.md) for navigation.
